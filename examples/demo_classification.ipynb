{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¯ Image Classification Demo\n",
                "\n",
                "Use classic image features with scikit-learn for image classification.\n",
                "\n",
                "We'll use a subset of **CIFAR-10** to demonstrate feature extraction and classification.\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kelkalot/imagefeatures/blob/main/examples/demo_classification.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q git+https://github.com/kelkalot/imagefeatures.git\n",
                "!pip install -q scikit-learn matplotlib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load CIFAR-10 Subset\n",
                "\n",
                "We'll use a small subset (500 images) for quick demonstration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import urllib.request\n",
                "import pickle\n",
                "import tarfile\n",
                "import os\n",
                "\n",
                "# Download CIFAR-10 if needed\n",
                "cifar_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
                "cifar_file = 'cifar-10-python.tar.gz'\n",
                "\n",
                "if not os.path.exists('cifar-10-batches-py'):\n",
                "    print('Downloading CIFAR-10...')\n",
                "    urllib.request.urlretrieve(cifar_url, cifar_file)\n",
                "    with tarfile.open(cifar_file, 'r:gz') as tar:\n",
                "        tar.extractall()\n",
                "    os.remove(cifar_file)\n",
                "    print('Done!')\n",
                "\n",
                "# Load one batch\n",
                "def load_cifar_batch(filename):\n",
                "    with open(filename, 'rb') as f:\n",
                "        data = pickle.load(f, encoding='bytes')\n",
                "    return data[b'data'], data[b'labels']\n",
                "\n",
                "X_raw, y = load_cifar_batch('cifar-10-batches-py/data_batch_1')\n",
                "\n",
                "# Reshape to images (N, 32, 32, 3)\n",
                "X_images = X_raw.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
                "\n",
                "# Use subset for speed\n",
                "n_samples = 500\n",
                "X_images = X_images[:n_samples]\n",
                "y = np.array(y[:n_samples])\n",
                "\n",
                "# Class names\n",
                "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
                "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
                "\n",
                "print(f'Loaded {len(X_images)} images')\n",
                "print(f'Image shape: {X_images[0].shape}')\n",
                "print(f'Classes: {len(set(y))} ({len(class_names)} total)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Display Sample Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    idx = np.where(y == i)[0][0]  # First image of each class\n",
                "    ax.imshow(X_images[idx])\n",
                "    ax.set_title(class_names[i])\n",
                "    ax.axis('off')\n",
                "plt.suptitle('CIFAR-10 Sample (one per class)', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Extract Features\n",
                "\n",
                "We'll extract a compact set of features suitable for small images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from imagefeatures.features import (\n",
                "    ColorHistogram, ColorMoments, LocalBinaryPatterns,\n",
                "    Gabor, Tamura, CEDD, Haralick, HuMoments\n",
                ")\n",
                "\n",
                "# Define feature extractors (compact features for 32x32 images)\n",
                "extractors = [\n",
                "    ColorHistogram(),    # 64d - color distribution\n",
                "    ColorMoments(),      # 9d  - color statistics\n",
                "    LocalBinaryPatterns(), # 256d - texture\n",
                "    Gabor(),             # 48d - multi-scale texture\n",
                "    Haralick(),          # 6d  - GLCM texture\n",
                "    HuMoments(),         # 7d  - shape\n",
                "]\n",
                "\n",
                "total_dim = sum(e.dim for e in extractors)\n",
                "print(f'Feature extractors: {len(extractors)}')\n",
                "print(f'Total dimensions: {total_dim}')\n",
                "for e in extractors:\n",
                "    print(f'  - {e.name}: {e.dim}d')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features from all images\n",
                "print(f'Extracting features from {len(X_images)} images...')\n",
                "\n",
                "X_features = []\n",
                "for i, img in enumerate(X_images):\n",
                "    if i % 100 == 0:\n",
                "        print(f'  Processing image {i}/{len(X_images)}')\n",
                "    \n",
                "    features = []\n",
                "    for extractor in extractors:\n",
                "        extractor.extract(img)\n",
                "        features.append(extractor.get_feature_vector())\n",
                "    \n",
                "    X_features.append(np.concatenate(features))\n",
                "\n",
                "X_features = np.array(X_features)\n",
                "print(f'\\nFeature matrix shape: {X_features.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_features, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Normalize features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f'Training set: {len(X_train)} images')\n",
                "print(f'Test set: {len(X_test)} images')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train Classifiers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "classifiers = {\n",
                "    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', C=10, gamma='scale'),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
                "}\n",
                "\n",
                "results = {}\n",
                "\n",
                "for name, clf in classifiers.items():\n",
                "    print(f'Training {name}...')\n",
                "    clf.fit(X_train_scaled, y_train)\n",
                "    y_pred = clf.predict(X_test_scaled)\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    results[name] = acc\n",
                "    print(f'  Accuracy: {acc:.2%}\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Compare Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Bar chart of accuracies\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "names = list(results.keys())\n",
                "accs = list(results.values())\n",
                "colors = plt.cm.viridis(np.linspace(0.3, 0.8, len(names)))\n",
                "\n",
                "bars = ax.barh(names, accs, color=colors)\n",
                "ax.set_xlim(0, 1)\n",
                "ax.set_xlabel('Accuracy')\n",
                "ax.set_title('CIFAR-10 Classification Results (500 samples)')\n",
                "\n",
                "# Add value labels\n",
                "for bar, acc in zip(bars, accs):\n",
                "    ax.text(acc + 0.02, bar.get_y() + bar.get_height()/2, \n",
                "            f'{acc:.1%}', va='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f'\\nBest classifier: {max(results, key=results.get)} ({max(results.values()):.1%})')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Detailed Classification Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use best classifier for detailed report\n",
                "best_clf_name = max(results, key=results.get)\n",
                "best_clf = classifiers[best_clf_name]\n",
                "y_pred = best_clf.predict(X_test_scaled)\n",
                "\n",
                "print(f'Classification Report ({best_clf_name}):\\n')\n",
                "print(classification_report(y_test, y_pred, target_names=class_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "im = ax.imshow(cm, cmap='Blues')\n",
                "\n",
                "# Labels\n",
                "ax.set_xticks(range(10))\n",
                "ax.set_yticks(range(10))\n",
                "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
                "ax.set_yticklabels(class_names)\n",
                "ax.set_xlabel('Predicted')\n",
                "ax.set_ylabel('True')\n",
                "ax.set_title(f'Confusion Matrix ({best_clf_name})')\n",
                "\n",
                "# Add values\n",
                "for i in range(10):\n",
                "    for j in range(10):\n",
                "        color = 'white' if cm[i, j] > cm.max()/2 else 'black'\n",
                "        ax.text(j, i, cm[i, j], ha='center', va='center', color=color)\n",
                "\n",
                "plt.colorbar(im, fraction=0.046)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Feature Importance (Random Forest)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importances from Random Forest\n",
                "rf = classifiers['Random Forest']\n",
                "importances = rf.feature_importances_\n",
                "\n",
                "# Group by feature extractor\n",
                "feature_groups = {}\n",
                "idx = 0\n",
                "for extractor in extractors:\n",
                "    dim = extractor.dim\n",
                "    group_importance = np.sum(importances[idx:idx+dim])\n",
                "    feature_groups[extractor.name] = group_importance\n",
                "    idx += dim\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "names = list(feature_groups.keys())\n",
                "values = list(feature_groups.values())\n",
                "colors = plt.cm.Spectral(np.linspace(0.1, 0.9, len(names)))\n",
                "\n",
                "bars = ax.barh(names, values, color=colors)\n",
                "ax.set_xlabel('Total Feature Importance')\n",
                "ax.set_title('Feature Group Importance (Random Forest)')\n",
                "\n",
                "for bar, val in zip(bars, values):\n",
                "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
                "            f'{val:.2f}', va='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Key Takeaways\n",
                "\n",
                "- **Classic features work!** Even with 32x32 images, we can achieve reasonable accuracy.\n",
                "- **Feature combination matters**: Mixing color, texture, and shape features improves results.\n",
                "- **SVM often works best** with classic features due to the normalized feature space.\n",
                "- **Random Forest** provides valuable feature importance insights.\n",
                "\n",
                "### Tips for Better Results:\n",
                "1. **More features**: Add CEDD, FCTH, PHOG for richer representation\n",
                "2. **Larger dataset**: Use full CIFAR-10 (50,000 images)\n",
                "3. **Feature selection**: Remove low-importance features\n",
                "4. **Hyperparameter tuning**: Use GridSearchCV for optimal parameters"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}